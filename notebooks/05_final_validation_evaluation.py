# notebooks/05_final_validation_evaluation.py
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import (
    train_test_split, cross_val_score, TimeSeriesSplit, 
    validation_curve, learning_curve
)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    mean_absolute_error, mean_squared_error, r2_score, 
    mean_absolute_percentage_error, classification_report, confusion_matrix
)
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import xgboost as xgb
import lightgbm as lgb

def load_optimized_data():
    """Cargar datos y resultados de optimizaci√≥n"""
    try:
        # Datos procesados
        df = pd.read_csv('data/processed/features_engineered.csv')
        
        # Resultados de optimizaci√≥n
        optimization_results = pd.read_csv('results/optimization_results.csv')
        
        # Consensus ranking de features
        consensus_ranking = pd.read_csv('results/feature_consensus_ranking.csv', index_col=0)
        
        print(f"‚úÖ Datos cargados: {df.shape}")
        print(f"‚úÖ Modelos optimizados: {len(optimization_results)}")
        
        return df, optimization_results, consensus_ranking
        
    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("Ejecuta primero 04_feature_analysis_optimization.py")
        return None, None, None

def prepare_temporal_validation_data(df):
    """Preparar datos para validaci√≥n temporal"""
    print("\nüìÖ PREPARANDO VALIDACI√ìN TEMPORAL")
    print("-" * 30)
    
    # Convertir fechas si existen
    if 'fecha_recuento' in df.columns:
        df['fecha_recuento'] = pd.to_datetime(df['fecha_recuento'], errors='coerce')
    else:
        # Generar fechas sint√©ticas basadas en dias_desde_recuento
        base_date = datetime.now()
        df['fecha_recuento'] = base_date - pd.to_timedelta(df['dias_desde_recuento'], unit='D')
    
    # Ordenar por fecha
    df_sorted = df.sort_values('fecha_recuento').reset_index(drop=True)
    
    # Crear splits temporales
    # 60% train, 20% validation, 20% test
    n_total = len(df_sorted)
    n_train = int(n_total * 0.6)
    n_val = int(n_total * 0.8)
    
    df_train = df_sorted[:n_train].copy()
    df_val = df_sorted[n_train:n_val].copy()
    df_test = df_sorted[n_val:].copy()
    
    print(f"üìä Split temporal:")
    print(f"  ‚Ä¢ Train: {len(df_train):,} registros ({df_train['fecha_recuento'].min().date()} a {df_train['fecha_recuento'].max().date()})")
    print(f"  ‚Ä¢ Validation: {len(df_val):,} registros ({df_val['fecha_recuento'].min().date()} a {df_val['fecha_recuento'].max().date()})")
    print(f"  ‚Ä¢ Test: {len(df_test):,} registros ({df_test['fecha_recuento'].min().date()} a {df_test['fecha_recuento'].max().date()})")
    
    return df_train, df_val, df_test

def get_top_features(consensus_ranking, n_features=15):
    """Obtener top features del consensus ranking"""
    top_features = consensus_ranking.sort_values('mean_rank').head(n_features).index.tolist()
    print(f"üéØ Usando top {n_features} features:")
    for i, feature in enumerate(top_features, 1):
        rank = consensus_ranking.loc[feature, 'mean_rank']
        print(f"  {i:2d}. {feature:25s} (rank: {rank:.1f})")
    
    return top_features

def temporal_cross_validation(X, y, model, task_type='classification', n_splits=5):
    """Validaci√≥n cruzada temporal"""
    print(f"\n‚è∞ VALIDACI√ìN CRUZADA TEMPORAL - {task_type.upper()}")
    print("-" * 30)
    
    # TimeSeriesSplit para validaci√≥n temporal
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    scores = []
    fold_details = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
        
        # Entrenar modelo
        model.fit(X_train_fold, y_train_fold)
        
        # Predicciones
        y_pred = model.predict(X_val_fold)
        
        # M√©tricas seg√∫n tipo de tarea
        if task_type == 'classification':
            score = f1_score(y_val_fold, y_pred)
            metric_name = 'F1-Score'
        else:  # regression
            y_pred = np.maximum(0, y_pred)  # No permitir predicciones negativas
            score = r2_score(y_val_fold, y_pred)
            metric_name = 'R¬≤'
        
        scores.append(score)
        fold_details.append({
            'fold': fold,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'score': score
        })
        
        print(f"  Fold {fold}: {metric_name} = {score:.3f} (train: {len(train_idx):,}, val: {len(val_idx):,})")
    
    print(f"\nüìä Resultado CV Temporal:")
    print(f"  ‚Ä¢ {metric_name} promedio: {np.mean(scores):.3f} (¬±{np.std(scores):.3f})")
    print(f"  ‚Ä¢ Rango: {np.min(scores):.3f} - {np.max(scores):.3f}")
    
    return scores, fold_details

def detailed_error_analysis(y_true, y_pred, task_type='regression', segment_data=None):
    """An√°lisis detallado de errores"""
    print(f"\nüîç AN√ÅLISIS DETALLADO DE ERRORES - {task_type.upper()}")
    print("-" * 30)
    
    if task_type == 'regression':
        # Asegurar predicciones no negativas
        y_pred = np.maximum(0, y_pred)
        
        # Calcular errores
        errors = y_true - y_pred
        abs_errors = np.abs(errors)
        pct_errors = np.abs(errors) / np.maximum(y_true, 1) * 100
        
        # Estad√≠sticas b√°sicas
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        mape = np.mean(pct_errors)
        
        print(f"üìà M√âTRICAS GENERALES:")
        print(f"  ‚Ä¢ MAE: {mae:.2f}")
        print(f"  ‚Ä¢ RMSE: {rmse:.2f}")
        print(f"  ‚Ä¢ R¬≤: {r2:.3f}")
        print(f"  ‚Ä¢ MAPE: {mape:.1f}%")
        
        # An√°lisis de distribuci√≥n de errores
        print(f"\nüìä DISTRIBUCI√ìN DE ERRORES:")
        print(f"  ‚Ä¢ Error medio: {np.mean(errors):.2f}")
        print(f"  ‚Ä¢ Error mediano: {np.median(errors):.2f}")
        print(f"  ‚Ä¢ Desviaci√≥n est√°ndar: {np.std(errors):.2f}")
        print(f"  ‚Ä¢ Percentil 90 error absoluto: {np.percentile(abs_errors, 90):.2f}")
        print(f"  ‚Ä¢ Percentil 95 error absoluto: {np.percentile(abs_errors, 95):.2f}")
        
        # Casos problem√°ticos
        high_error_threshold = np.percentile(abs_errors, 95)
        high_error_cases = abs_errors > high_error_threshold
        
        print(f"\nüö® CASOS CON ERRORES ALTOS (>P95):")
        print(f"  ‚Ä¢ N√∫mero de casos: {high_error_cases.sum():,} ({high_error_cases.sum()/len(y_true)*100:.1f}%)")
        print(f"  ‚Ä¢ Error promedio en estos casos: {abs_errors[high_error_cases].mean():.2f}")
        
        return {
            'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape,
            'errors': errors, 'abs_errors': abs_errors, 'pct_errors': pct_errors,
            'high_error_cases': high_error_cases
        }
        
    else:  # classification
        # M√©tricas de clasificaci√≥n
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        
        print(f"üéØ M√âTRICAS GENERALES:")
        print(f"  ‚Ä¢ Accuracy: {accuracy:.3f}")
        print(f"  ‚Ä¢ Precision: {precision:.3f}")
        print(f"  ‚Ä¢ Recall: {recall:.3f}")
        print(f"  ‚Ä¢ F1-Score: {f1:.3f}")
        
        # Matriz de confusi√≥n
        cm = confusion_matrix(y_true, y_pred)
        print(f"\nüìä MATRIZ DE CONFUSI√ìN:")
        print(f"  ‚Ä¢ Verdaderos Negativos: {cm[0,0]:,}")
        print(f"  ‚Ä¢ Falsos Positivos: {cm[0,1]:,}")
        print(f"  ‚Ä¢ Falsos Negativos: {cm[1,0]:,}")
        print(f"  ‚Ä¢ Verdaderos Positivos: {cm[1,1]:,}")
        
        return {
            'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,
            'confusion_matrix': cm
        }

def segment_analysis(df_test, y_true, y_pred, task_type='regression'):
    """An√°lisis por segmentos (alias, tiendas, etc.)"""
    print(f"\nüè™ AN√ÅLISIS POR SEGMENTOS")
    print("-" * 30)
    
    results = {}
    
    # An√°lisis por Alias
    if 'ID_ALIAS' in df_test.columns:
        print("üìä An√°lisis por ALIAS:")
        alias_results = []
        
        for alias_id in df_test['ID_ALIAS'].unique()[:10]:  # Top 10 alias con m√°s datos
            mask = df_test['ID_ALIAS'] == alias_id
            if mask.sum() < 20:  # Skip alias con pocos datos
                continue
                
            y_true_alias = y_true[mask]
            y_pred_alias = y_pred[mask]
            
            if task_type == 'regression':
                y_pred_alias = np.maximum(0, y_pred_alias)
                mae = mean_absolute_error(y_true_alias, y_pred_alias)
                r2 = r2_score(y_true_alias, y_pred_alias)
                alias_results.append({
                    'ID_ALIAS': alias_id,
                    'n_records': mask.sum(),
                    'MAE': mae,
                    'R2': r2
                })
            else:
                f1 = f1_score(y_true_alias, y_pred_alias)
                accuracy = accuracy_score(y_true_alias, y_pred_alias)
                alias_results.append({
                    'ID_ALIAS': alias_id,
                    'n_records': mask.sum(),
                    'F1': f1,
                    'Accuracy': accuracy
                })
        
        if alias_results:
            alias_df = pd.DataFrame(alias_results)
            if task_type == 'regression':
                alias_df = alias_df.sort_values('R2', ascending=False)
                print(f"  Top 5 Alias por R¬≤:")
                for _, row in alias_df.head().iterrows():
                    print(f"    Alias {row['ID_ALIAS']:3.0f}: R¬≤={row['R2']:.3f}, MAE={row['MAE']:.1f} ({row['n_records']:,} registros)")
            else:
                alias_df = alias_df.sort_values('F1', ascending=False)
                print(f"  Top 5 Alias por F1:")
                for _, row in alias_df.head().iterrows():
                    print(f"    Alias {row['ID_ALIAS']:3.0f}: F1={row['F1']:.3f}, Acc={row['Accuracy']:.3f} ({row['n_records']:,} registros)")
            
            results['alias_analysis'] = alias_df
    
    # An√°lisis por tama√±o de tienda
    if 'tienda_tama√±o' in df_test.columns:
        print(f"\nüìä An√°lisis por TAMA√ëO DE TIENDA:")
        for tama√±o in df_test['tienda_tama√±o'].unique():
            mask = df_test['tienda_tama√±o'] == tama√±o
            if mask.sum() < 50:  # Skip categor√≠as con pocos datos
                continue
                
            y_true_tam = y_true[mask]
            y_pred_tam = y_pred[mask]
            
            if task_type == 'regression':
                y_pred_tam = np.maximum(0, y_pred_tam)
                mae = mean_absolute_error(y_true_tam, y_pred_tam)
                r2 = r2_score(y_true_tam, y_pred_tam)
                print(f"  {tama√±o:10s}: R¬≤={r2:.3f}, MAE={mae:.1f} ({mask.sum():,} registros)")
            else:
                f1 = f1_score(y_true_tam, y_pred_tam)
                accuracy = accuracy_score(y_true_tam, y_pred_tam)
                print(f"  {tama√±o:10s}: F1={f1:.3f}, Acc={accuracy:.3f} ({mask.sum():,} registros)")
    
    # An√°lisis por nivel de urgencia
    if 'urgencia_reposicion' in df_test.columns:
        print(f"\nüìä An√°lisis por URGENCIA DE REPOSICI√ìN:")
        urgencia_map = {0: 'Normal', 1: 'Medio', 2: 'Alto', 3: 'Cr√≠tico'}
        
        for urgencia in sorted(df_test['urgencia_reposicion'].unique()):
            mask = df_test['urgencia_reposicion'] == urgencia
            if mask.sum() < 20:
                continue
                
            y_true_urg = y_true[mask]
            y_pred_urg = y_pred[mask]
            urgencia_name = urgencia_map.get(urgencia, f'Nivel_{urgencia}')
            
            if task_type == 'regression':
                y_pred_urg = np.maximum(0, y_pred_urg)
                mae = mean_absolute_error(y_true_urg, y_pred_urg)
                r2 = r2_score(y_true_urg, y_pred_urg)
                print(f"  {urgencia_name:8s}: R¬≤={r2:.3f}, MAE={mae:.1f} ({mask.sum():,} registros)")
            else:
                f1 = f1_score(y_true_urg, y_pred_urg)
                accuracy = accuracy_score(y_true_urg, y_pred_urg)
                print(f"  {urgencia_name:8s}: F1={f1:.3f}, Acc={accuracy:.3f} ({mask.sum():,} registros)")
    
    return results

def create_validation_visualizations(df_test, y_true_class, y_pred_class, y_true_reg, y_pred_reg, error_analysis_reg):
    """Crear visualizaciones de validaci√≥n"""
    print(f"\nüìä CREANDO VISUALIZACIONES DE VALIDACI√ìN")
    print("-" * 30)
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 12))
    
    # 1. Matriz de confusi√≥n - Clasificaci√≥n
    cm = confusion_matrix(y_true_class, y_pred_class)
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,0], cmap='Blues')
    axes[0,0].set_title('Matriz de Confusi√≥n\n(Necesita Reposici√≥n)')
    axes[0,0].set_xlabel('Predicci√≥n')
    axes[0,0].set_ylabel('Realidad')
    
    # 2. Distribuci√≥n de probabilidades - Clasificaci√≥n
    if hasattr(y_pred_class, 'predict_proba'):  # Si tenemos probabilidades
        # Esto requerir√≠a el modelo, por ahora skip
        pass
    else:
        # Mostrar distribuci√≥n de predicciones por clase real
        df_class_viz = pd.DataFrame({
            'real': y_true_class,
            'pred': y_pred_class
        })
        df_class_viz['correct'] = df_class_viz['real'] == df_class_viz['pred']
        df_class_viz['correct'].value_counts().plot(kind='bar', ax=axes[0,1])
        axes[0,1].set_title('Predicciones Correctas vs Incorrectas')
        axes[0,1].set_xlabel('¬øPredicci√≥n Correcta?')
        axes[0,1].set_ylabel('N√∫mero de casos')
    
    # 3. Predicciones vs Realidad - Regresi√≥n
    y_pred_reg_clean = np.maximum(0, y_pred_reg)
    axes[0,2].scatter(y_true_reg, y_pred_reg_clean, alpha=0.6, s=1)
    max_val = max(y_true_reg.max(), y_pred_reg_clean.max())
    axes[0,2].plot([0, max_val], [0, max_val], 'r--', alpha=0.7)
    axes[0,2].set_xlabel('Stock Real')
    axes[0,2].set_ylabel('Stock Predicho')
    axes[0,2].set_title('Predicciones vs Realidad\n(Regresi√≥n)')
    axes[0,2].grid(True, alpha=0.3)
    
    # 4. Distribuci√≥n de errores - Regresi√≥n
    errors = error_analysis_reg['errors']
    axes[0,3].hist(errors, bins=50, alpha=0.7, edgecolor='black')
    axes[0,3].axvline(0, color='red', linestyle='--', label='Error = 0')
    axes[0,3].set_xlabel('Error (Real - Predicho)')
    axes[0,3].set_ylabel('Frecuencia')
    axes[0,3].set_title('Distribuci√≥n de Errores')
    axes[0,3].legend()
    axes[0,3].grid(True, alpha=0.3)
    
    # 5. Errores absolutos por valor real
    abs_errors = error_analysis_reg['abs_errors']
    axes[1,0].scatter(y_true_reg, abs_errors, alpha=0.6, s=1)
    axes[1,0].set_xlabel('Stock Real')
    axes[1,0].set_ylabel('Error Absoluto')
    axes[1,0].set_title('Error Absoluto vs Stock Real')
    axes[1,0].grid(True, alpha=0.3)
    
    # 6. Errores porcentuales
    pct_errors = error_analysis_reg['pct_errors']
    # Filtrar outliers extremos para visualizaci√≥n
    pct_errors_clean = pct_errors[pct_errors <= np.percentile(pct_errors, 95)]
    axes[1,1].hist(pct_errors_clean, bins=50, alpha=0.7, edgecolor='black')
    axes[1,1].set_xlabel('Error Porcentual (%)')
    axes[1,1].set_ylabel('Frecuencia')
    axes[1,1].set_title('Distribuci√≥n Error Porcentual')
    axes[1,1].grid(True, alpha=0.3)
    
    # 7. Casos con errores altos
    high_error_cases = error_analysis_reg['high_error_cases']
    error_categories = ['Error Normal', 'Error Alto']
    error_counts = [(~high_error_cases).sum(), high_error_cases.sum()]
    axes[1,2].pie(error_counts, labels=error_categories, autopct='%1.1f%%')
    axes[1,2].set_title('Distribuci√≥n de Casos\npor Nivel de Error')
    
    # 8. Residuos vs predicciones
    axes[1,3].scatter(y_pred_reg_clean, errors, alpha=0.6, s=1)
    axes[1,3].axhline(0, color='red', linestyle='--')
    axes[1,3].set_xlabel('Stock Predicho')
    axes[1,3].set_ylabel('Residuos')
    axes[1,3].set_title('Residuos vs Predicciones')
    axes[1,3].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/final_validation_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def evaluate_best_models(df_train, df_val, df_test, top_features, optimization_results):
    """Evaluar los mejores modelos con validaci√≥n temporal"""
    print(f"\nüèÜ EVALUACI√ìN FINAL DE MEJORES MODELOS")
    print("="*50)
    
    # Preparar datos
    X_train = df_train[top_features].fillna(df_train[top_features].median())
    X_val = df_val[top_features].fillna(df_val[top_features].median())
    X_test = df_test[top_features].fillna(df_test[top_features].median())
    
    y_train_class = df_train['necesita_reposicion']
    y_val_class = df_val['necesita_reposicion']
    y_test_class = df_test['necesita_reposicion']
    
    y_train_reg = df_train['cantidad_a_reponer']
    y_val_reg = df_val['cantidad_a_reponer']
    y_test_reg = df_test['cantidad_a_reponer']
    
    # Para regresi√≥n, filtrar solo casos con reposici√≥n > 0
    reg_mask_train = y_train_reg > 0
    reg_mask_val = y_val_reg > 0
    reg_mask_test = y_test_reg > 0
    
    X_train_reg = X_train[reg_mask_train]
    X_val_reg = X_val[reg_mask_val]
    X_test_reg = X_test[reg_mask_test]
    
    y_train_reg_filtered = y_train_reg[reg_mask_train]
    y_val_reg_filtered = y_val_reg[reg_mask_val]
    y_test_reg_filtered = y_test_reg[reg_mask_test]
    
    print(f"üìä Datos preparados:")
    print(f"  ‚Ä¢ Clasificaci√≥n - Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}")
    print(f"  ‚Ä¢ Regresi√≥n - Train: {len(X_train_reg):,}, Val: {len(X_val_reg):,}, Test: {len(X_test_reg):,}")
    
    final_results = {}
    final_predictions = {}
    
    # Obtener mejores modelos de optimization_results
    best_models = optimization_results['model'].unique()
    
    for model_name in best_models:
        print(f"\nüîç Evaluando {model_name}...")
        
        # CLASIFICACI√ìN
        if model_name == 'Random Forest':
            model_class = RandomForestClassifier(
                n_estimators=200, max_depth=20, min_samples_split=2,
                random_state=42, n_jobs=-1
            )
        elif model_name == 'XGBoost':
            model_class = xgb.XGBClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                random_state=42, eval_metric='logloss', n_jobs=-1
            )
        elif model_name == 'LightGBM':
            model_class = lgb.LGBMClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                random_state=42, verbose=-1, n_jobs=-1
            )
        else:
            continue
        
        # Entrenar y evaluar clasificaci√≥n
        model_class.fit(X_train, y_train_class)
        y_pred_class = model_class.predict(X_test)
        
        # Validaci√≥n cruzada temporal
        cv_scores_class, _ = temporal_cross_validation(
            pd.concat([X_train, X_val]), 
            pd.concat([y_train_class, y_val_class]), 
            model_class, 
            'classification'
        )
        
        # An√°lisis de errores clasificaci√≥n
        error_analysis_class = detailed_error_analysis(
            y_test_class, y_pred_class, 'classification'
        )
        
        # REGRESI√ìN
        if model_name == 'Random Forest':
            model_reg = RandomForestRegressor(
                n_estimators=200, max_depth=20, min_samples_split=2,
                random_state=42, n_jobs=-1
            )
        elif model_name == 'XGBoost':
            model_reg = xgb.XGBRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                random_state=42, n_jobs=-1
            )
        elif model_name == 'LightGBM':
            model_reg = lgb.LGBMRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                random_state=42, verbose=-1, n_jobs=-1
            )
        
        # Entrenar y evaluar regresi√≥n
        model_reg.fit(X_train_reg, y_train_reg_filtered)
        y_pred_reg = model_reg.predict(X_test_reg)
        
        # Validaci√≥n cruzada temporal
        cv_scores_reg, _ = temporal_cross_validation(
            pd.concat([X_train_reg, X_val_reg]), 
            pd.concat([y_train_reg_filtered, y_val_reg_filtered]), 
            model_reg, 
            'regression'
        )
        
        # An√°lisis de errores regresi√≥n
        error_analysis_reg = detailed_error_analysis(
            y_test_reg_filtered, y_pred_reg, 'regression'
        )
        
        # An√°lisis por segmentos
        segment_results_class = segment_analysis(
            df_test, y_test_class, y_pred_class, 'classification'
        )
        
        segment_results_reg = segment_analysis(
            df_test.loc[reg_mask_test], y_test_reg_filtered, y_pred_reg, 'regression'
        )
        
        # Guardar resultados
        final_results[model_name] = {
            'classification': {
                'cv_scores': cv_scores_class,
                'error_analysis': error_analysis_class,
                'segment_analysis': segment_results_class
            },
            'regression': {
                'cv_scores': cv_scores_reg,
                'error_analysis': error_analysis_reg,
                'segment_analysis': segment_results_reg
            }
        }
        
        final_predictions[model_name] = {
            'classification': {
                'y_true': y_test_class,
                'y_pred': y_pred_class,
                'model': model_class
            },
            'regression': {
                'y_true': y_test_reg_filtered,
                'y_pred': y_pred_reg,
                'model': model_reg
            }
        }
    
    return final_results, final_predictions

def save_validation_results(final_results, final_predictions):
    """Guardar resultados de validaci√≥n final"""
    print(f"\nüíæ GUARDANDO RESULTADOS DE VALIDACI√ìN FINAL")
    print("-" * 30)
    
    # Crear resumen de resultados
    summary_results = []
    
    for model_name, results in final_results.items():
        # Clasificaci√≥n
        class_cv = np.mean(results['classification']['cv_scores'])
        class_f1 = results['classification']['error_analysis']['f1']
        class_accuracy = results['classification']['error_analysis']['accuracy']
        
        # Regresi√≥n
        reg_cv = np.mean(results['regression']['cv_scores'])
        reg_r2 = results['regression']['error_analysis']['r2']
        reg_mae = results['regression']['error_analysis']['mae']
        
        summary_results.append({
            'model': model_name,
            'classification_cv_f1': class_cv,
            'classification_test_f1': class_f1,
            'classification_test_accuracy': class_accuracy,
            'regression_cv_r2': reg_cv,
            'regression_test_r2': reg_r2,
            'regression_test_mae': reg_mae
        })
    
    # Guardar resumen
    summary_df = pd.DataFrame(summary_results)
    summary_df.to_csv('results/final_validation_summary.csv', index=False)
    
    # Guardar predicciones para an√°lisis posterior
    for model_name, predictions in final_predictions.items():
        # Guardar predicciones de clasificaci√≥n
       class_pred_df = pd.DataFrame({
           'y_true': predictions['classification']['y_true'],
           'y_pred': predictions['classification']['y_pred']
       })
       class_pred_df.to_csv(f'results/predictions_classification_{model_name.replace(" ", "_")}.csv', index=False)
       
       # Guardar predicciones de regresi√≥n
       reg_pred_df = pd.DataFrame({
           'y_true': predictions['regression']['y_true'],
           'y_pred': predictions['regression']['y_pred']
       })
       reg_pred_df.to_csv(f'results/predictions_regression_{model_name.replace(" ", "_")}.csv', index=False)
   
    print("‚úÖ Archivos guardados:")
    print(" ‚Ä¢ results/final_validation_summary.csv")
    print(" ‚Ä¢ results/predictions_classification_*.csv")
    print(" ‚Ä¢ results/predictions_regression_*.csv")

def print_final_validation_summary(final_results):
   """Imprimir resumen final de validaci√≥n"""
   print(f"\nüèÜ RESUMEN FINAL DE VALIDACI√ìN")
   print("="*60)
   
   for model_name, results in final_results.items():
       print(f"\nü§ñ {model_name.upper()}")
       print("-" * 40)
       
       # Clasificaci√≥n
       class_results = results['classification']
       cv_f1 = np.mean(class_results['cv_scores'])
       cv_f1_std = np.std(class_results['cv_scores'])
       test_f1 = class_results['error_analysis']['f1']
       test_acc = class_results['error_analysis']['accuracy']
       
       print(f"üéØ CLASIFICACI√ìN (¬øNecesita reposici√≥n?):")
       print(f"  ‚Ä¢ CV F1-Score: {cv_f1:.3f} (¬±{cv_f1_std:.3f})")
       print(f"  ‚Ä¢ Test F1-Score: {test_f1:.3f}")
       print(f"  ‚Ä¢ Test Accuracy: {test_acc:.3f}")
       
       # Regresi√≥n
       reg_results = results['regression']
       cv_r2 = np.mean(reg_results['cv_scores'])
       cv_r2_std = np.std(reg_results['cv_scores'])
       test_r2 = reg_results['error_analysis']['r2']
       test_mae = reg_results['error_analysis']['mae']
       test_mape = reg_results['error_analysis']['mape']
       
       print(f"\nüìà REGRESI√ìN (¬øCu√°nto reponer?):")
       print(f"  ‚Ä¢ CV R¬≤: {cv_r2:.3f} (¬±{cv_r2_std:.3f})")
       print(f"  ‚Ä¢ Test R¬≤: {test_r2:.3f}")
       print(f"  ‚Ä¢ Test MAE: {test_mae:.2f} unidades")
       print(f"  ‚Ä¢ Test MAPE: {test_mape:.1f}%")
   
   # Determinar mejor modelo general
   print(f"\nü•á MEJOR MODELO GENERAL:")
   best_model = None
   best_score = -np.inf
   
   for model_name, results in final_results.items():
       # Puntuaci√≥n combinada (normalizada)
       f1_score = results['classification']['error_analysis']['f1']
       r2_score = results['regression']['error_analysis']['r2']
       combined_score = (f1_score + max(0, r2_score)) / 2  # Promedio simple
       
       print(f"  ‚Ä¢ {model_name}: {combined_score:.3f} (F1: {f1_score:.3f}, R¬≤: {r2_score:.3f})")
       
       if combined_score > best_score:
           best_score = combined_score
           best_model = model_name
   
   print(f"\nüèÜ GANADOR: {best_model} (Score: {best_score:.3f})")

def main():
   print("üöÄ INICIANDO VALIDACI√ìN FINAL Y EVALUACI√ìN PROFUNDA")
   print("="*60)
   
   # Cargar datos y resultados
   df, optimization_results, consensus_ranking = load_optimized_data()
   if df is None:
       return
   
   # Preparar split temporal
   df_train, df_val, df_test = prepare_temporal_validation_data(df)
   
   # Obtener top features
   top_features = get_top_features(consensus_ranking, n_features=15)
   
   # Evaluaci√≥n final de mejores modelos
   final_results, final_predictions = evaluate_best_models(
       df_train, df_val, df_test, top_features, optimization_results
   )
   
   # Crear visualizaciones de validaci√≥n
   if final_predictions:
       # Usar el primer modelo para visualizaciones
       first_model = list(final_predictions.keys())[0]
       y_true_class = final_predictions[first_model]['classification']['y_true']
       y_pred_class = final_predictions[first_model]['classification']['y_pred']
       y_true_reg = final_predictions[first_model]['regression']['y_true']
       y_pred_reg = final_predictions[first_model]['regression']['y_pred']
       
       error_analysis_reg = final_results[first_model]['regression']['error_analysis']
       
       create_validation_visualizations(
           df_test, y_true_class, y_pred_class, y_true_reg, y_pred_reg, error_analysis_reg
       )
   
   # Guardar resultados
   save_validation_results(final_results, final_predictions)
   
   # Imprimir resumen final
   print_final_validation_summary(final_results)
   
   print(f"\n‚úÖ VALIDACI√ìN FINAL COMPLETADA")
   print(f"üìÅ Archivos generados:")
   print(f"  ‚Ä¢ results/final_validation_summary.csv")
   print(f"  ‚Ä¢ results/predictions_*.csv")
   print(f"  ‚Ä¢ results/plots/final_validation_analysis.png")
   
   print(f"\nüéØ PR√ìXIMO PASO: 06_final_models_predictions.py")
   print(f"  ‚Ä¢ Entrenar modelos finales con mejores configuraciones")
   print(f"  ‚Ä¢ Generar predicciones de stock te√≥rico")
   print(f"  ‚Ä¢ Crear sistema de ensemble")
   
   return final_results, final_predictions, top_features

if __name__ == "__main__":
   final_results, final_predictions, top_features = main()